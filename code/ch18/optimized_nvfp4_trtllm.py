"""optimized_nvfp4_trtllm.py - NVFP4/TRT-LLM integration path.

If TensorRT-LLM is present and an engine path is provided via TRT_LLM_ENGINE,
run a small inference; otherwise fall back to a Transformer Engine NVFP4 demo
or report SKIPPED with a clear message.
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Optional
import os

import torch
import torch.nn as nn

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from core.harness.benchmark_harness import BaseBenchmark, WorkloadMetadata  # noqa: E402
from core.benchmark.verification_mixin import VerificationPayloadMixin
from core.profiling.nvtx_helper import get_nvtx_enabled, nvtx_range  # noqa: E402


class NVFP4TRTLLMBenchmark(VerificationPayloadMixin, BaseBenchmark):
    def __init__(self) -> None:
        super().__init__()
        self.linear: Optional[nn.Linear] = None
        self.inputs: Optional[torch.Tensor] = None
        self._workload = WorkloadMetadata(tokens_per_iteration=0.0)
        self._stack_available = False
        self.output: Optional[torch.Tensor] = None
        self.graph = None
        self._trt_runner = None
        self._verification_payload = None

    def setup(self) -> None:
        # TensorRT-LLM path first, with optional CUDA Graph capture.
        engine_path = os.getenv("TRT_LLM_ENGINE")
        trt_exc: Optional[Exception] = None
        try:
            import tensorrt_llm  # type: ignore
            from tensorrt_llm.runtime import ModelRunner  # type: ignore
            if engine_path is not None:
                self._stack_available = True
                self._trt_runner = ModelRunner.from_engine(engine_path)
                self.inputs = torch.randint(0, 1000, (1, 32), device=self.device, dtype=torch.int32)
                try:
                    stream = torch.cuda.Stream()
                    torch.cuda.synchronize()
                    self.graph = torch.cuda.CUDAGraph()
                    with torch.cuda.graph(self.graph, stream=stream):
                        self._trt_runner.generate(self.inputs)  # type: ignore[attr-defined]
                except Exception:
                    self.graph = None
                return
        except Exception as exc:  # pragma: no cover - optional dependency
            trt_exc = exc
        trt_msg = f"{trt_exc}" if trt_exc is not None else "TensorRT-LLM unavailable"

        try:
            import transformer_engine.pytorch as te  # type: ignore
            from transformer_engine.pytorch import fp8_autocast  # noqa: F401
            self._stack_available = True
            self._te = te
        except Exception as exc:  # pragma: no cover - optional dependency
            raise RuntimeError(f"SKIPPED: NVFP4 stack not available ({trt_msg})") from exc

        self.linear = nn.Linear(1024, 1024, bias=False).to(self.device).to(torch.float16)
        self.inputs = torch.randn(32, 1024, device=self.device, dtype=torch.float16)
        torch.cuda.synchronize(self.device)

    def benchmark_fn(self) -> Optional[dict]:
        if not self._stack_available:
            raise RuntimeError("SKIPPED: NVFP4 stack not available")

        enable_nvtx = get_nvtx_enabled(self.get_config())

        # TensorRT-LLM path if runner exists.
        if hasattr(self, "_trt_runner") and self.inputs is not None:
            with nvtx_range("nvfp4_trtllm_engine", enable=enable_nvtx):
                outputs = self._trt_runner.generate(self.inputs)  # type: ignore[attr-defined]
                try:
                    first = outputs[0] if isinstance(outputs, (list, tuple)) else outputs
                    self.output = first if isinstance(first, torch.Tensor) else torch.as_tensor(first)
                except Exception:
                    self.output = torch.tensor([float(len(outputs))], device=self.device)
            torch.cuda.synchronize(self.device)
            if self.output is None:
                raise RuntimeError("TRT-LLM generate did not produce output")
            return {}

        if self.linear is None or self.inputs is None:
            raise RuntimeError("SKIPPED: NVFP4 linear model not initialized")

        with nvtx_range("nvfp4_te_fp8", enable=enable_nvtx):
            try:
                from transformer_engine.pytorch import fp8_autocast  # type: ignore
                with fp8_autocast():
                    self.output = self.linear(self.inputs)
            except Exception:
                self.output = self.linear(self.inputs)
        torch.cuda.synchronize(self.device)
        if self.output is None:
            raise RuntimeError("benchmark_fn() must produce output")
        return {}

    def capture_verification_payload(self) -> None:
        self._set_verification_payload(
            inputs={"inputs": self.inputs},
            output=self.output,
            batch_size=self.inputs.shape[0],
            parameter_count=sum(p.numel() for p in self.linear.parameters()) if self.linear is not None else 0,
            precision_flags={"fp16": self.output.dtype == torch.float16, "bf16": self.output.dtype == torch.bfloat16, "fp8": False, "tf32": torch.backends.cuda.matmul.allow_tf32},
            output_tolerance=(0.1, 1.0),
        )

    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload

def get_benchmark() -> BaseBenchmark:
    return NVFP4TRTLLMBenchmark()
