"""Torchrun wrapper for benchmark launches.

This wrapper exists to enforce harness-level invariants inside torchrun-launched
multi-process benchmarks.

Currently enforced:
- RNG seed immutability: benchmarks must not reseed away from the harness-
  configured seeds (default seed=42).

The harness launches torchrun with this wrapper as the entrypoint and passes the
original benchmark script path + args through unchanged.
"""

from __future__ import annotations

import argparse
import os
import random
import runpy
import sys
from pathlib import Path
from typing import Optional

import numpy as np
import torch

from core.harness.backend_policy import BackendPolicyName, apply_backend_policy


def _apply_backend_policy(deterministic: bool) -> None:
    apply_backend_policy(BackendPolicyName.PERFORMANCE, deterministic)


def _set_seeds(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def _parse_int_env(name: str) -> Optional[int]:
    value = os.environ.get(name)
    if value is None or value == "":
        return None
    try:
        return int(value)
    except ValueError as exc:
        raise RuntimeError(f"Invalid integer for {name}: {value!r}") from exc


def _resolve_local_rank() -> int:
    for key in ("LOCAL_RANK", "RANK"):
        value = os.environ.get(key)
        if value is not None and value != "":
            try:
                return int(value)
            except ValueError as exc:
                raise RuntimeError(f"Invalid {key} value: {value!r}") from exc
    return 0


def _run_target_script(script_path: Path, argv: list[str]) -> None:
    previous_argv = sys.argv
    previous_path0: Optional[str] = None
    try:
        sys.argv = [str(script_path), *argv]
        if sys.path:
            previous_path0 = sys.path[0]
            sys.path[0] = str(script_path.parent)
        else:
            sys.path.insert(0, str(script_path.parent))
        runpy.run_path(str(script_path), run_name="__main__")
    finally:
        sys.argv = previous_argv
        if previous_path0 is None:
            if sys.path and sys.path[0] == str(script_path.parent):
                sys.path.pop(0)
        else:
            if sys.path:
                sys.path[0] = previous_path0


def main(argv: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(add_help=True)
    parser.add_argument(
        "--aisp-target-script",
        required=True,
        help="Path to the benchmark script to execute under torchrun.",
    )
    parser.add_argument(
        "--aisp-expected-torch-seed",
        required=True,
        type=int,
        help="Expected torch.initial_seed() after benchmark completes.",
    )
    parser.add_argument(
        "--aisp-expected-cuda-seed",
        required=False,
        type=int,
        help="Expected torch.cuda.initial_seed() after benchmark completes (if CUDA is available).",
    )
    parser.add_argument(
        "--aisp-deterministic",
        action="store_true",
        help="Enable deterministic algorithms (mirrors harness deterministic mode).",
    )
    args, remainder = parser.parse_known_args(argv)

    script_path = Path(args.aisp_target_script).resolve()
    if not script_path.exists():
        raise FileNotFoundError(f"Target script not found: {script_path}")

    _apply_backend_policy(bool(args.aisp_deterministic))
    _set_seeds(int(args.aisp_expected_torch_seed))

    expected_torch_seed = int(args.aisp_expected_torch_seed)
    expected_cuda_seed: Optional[int] = args.aisp_expected_cuda_seed

    lock_requested = os.environ.get("AISP_LOCK_GPU_CLOCKS") == "1"
    ramp_requested = os.environ.get("AISP_RAMP_GPU_CLOCKS", "1") == "1"
    local_rank = _resolve_local_rank()

    if torch.cuda.is_available():
        torch.cuda.set_device(local_rank)

    if lock_requested and torch.cuda.is_available():
        from core.harness.benchmark_harness import lock_gpu_clocks, ramp_gpu_clocks

        lock_ctx = lock_gpu_clocks(
            device=local_rank,
            sm_clock_mhz=_parse_int_env("AISP_GPU_SM_CLOCK_MHZ"),
            mem_clock_mhz=_parse_int_env("AISP_GPU_MEM_CLOCK_MHZ"),
        )
        with lock_ctx:
            if ramp_requested:
                ramp_gpu_clocks(device=local_rank)
            _run_target_script(script_path, remainder)
    else:
        _run_target_script(script_path, remainder)

    current_torch_seed = int(torch.initial_seed())
    if current_torch_seed != expected_torch_seed:
        raise RuntimeError(
            "Seed mutation detected during torchrun benchmark execution. "
            f"Expected torch.initial_seed()={expected_torch_seed}, got {current_torch_seed}. "
            "Benchmarks MUST NOT reseed; rely on harness-configured seeds."
        )

    if torch.cuda.is_available():
        if expected_cuda_seed is None:
            raise RuntimeError(
                "torch.cuda.is_available() is true but --aisp-expected-cuda-seed was not provided."
            )
        current_cuda_seed = int(torch.cuda.initial_seed())
        if current_cuda_seed != int(expected_cuda_seed):
            raise RuntimeError(
                "CUDA seed mutation detected during torchrun benchmark execution. "
                f"Expected torch.cuda.initial_seed()={int(expected_cuda_seed)}, got {current_cuda_seed}. "
                "Benchmarks MUST NOT reseed; rely on harness-configured seeds."
            )


if __name__ == "__main__":
    main()
