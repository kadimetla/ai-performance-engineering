#!/usr/bin/env python3
"""Generate the C++ arch_detection.cuh header from canonical capability data."""

from __future__ import annotations

from pathlib import Path
from typing import Dict, Tuple

repo_root = Path(__file__).resolve().parents[2]
if str(repo_root) not in __import__("sys").path:
    __import__("sys").path.insert(0, str(repo_root))

from core.harness.hardware_capabilities import all_capability_records, refresh_capability_cache


HEADER_PATH = repo_root / "core" / "common" / "headers" / "arch_detection.cuh"


def parse_sm_tag(sm_tag: str) -> Tuple[int, int]:
    digits = sm_tag.replace("sm_", "")
    value = int(digits)
    major = value // 10
    minor = value % 10
    return major, minor


def tensor_core_generation(architecture: str) -> str:
    arch = (architecture or "other").lower()
    if "blackwell" in arch:
        return "ArchitectureLimits::TensorCoreGeneration::Blackwell"
    if "hopper" in arch:
        return "ArchitectureLimits::TensorCoreGeneration::Hopper"
    if "ampere" in arch:
        return "ArchitectureLimits::TensorCoreGeneration::Ampere"
    return "ArchitectureLimits::TensorCoreGeneration::None"


def tensor_core_tiles(architecture: str) -> Tuple[int, int, int]:
    arch = (architecture or "other").lower()
    if "blackwell" in arch or "hopper" in arch:
        return 128, 128, 64
    if "ampere" in arch:
        return 64, 64, 32
    return 32, 32, 16


def format_bool(value: bool) -> str:
    return "true" if value else "false"


def emit_capability_entries(records: Dict[str, Dict[str, object]]) -> str:
    lines = []
    for key in sorted(records.keys()):
        record = records[key]
        major, minor = parse_sm_tag(key)
        tma = record.get("tma", {}) or {}
        cluster = record.get("cluster", {}) or {}

        max_1d = int(tma.get("max_1d", 0) or 0)
        max_2d_w = int(tma.get("max_2d_width", 0) or 0)
        max_2d_h = int(tma.get("max_2d_height", 0) or 0)

        supports_clusters = bool(cluster.get("supports_clusters", False))
        has_dsmem = bool(cluster.get("has_dsmem", False))
        max_cluster_size = int(cluster.get("max_cluster_size", 1))

        tensor_gen = tensor_core_generation(record.get("architecture", ""))
        tile_m, tile_n, tile_k = tensor_core_tiles(record.get("architecture", ""))

        has_grace = bool(record.get("grace_coherence", False))
        has_nvlink_c2c = bool(record.get("nvlink_c2c", False))

        entry = f"""    {{
        {major}, {minor},
        {{{max_1d}, {max_2d_w}, {max_2d_h}}},
        {format_bool(supports_clusters)},
        {format_bool(has_dsmem)},
        {max_cluster_size},
        {tensor_gen},
        {tile_m}, {tile_n}, {tile_k},
        {format_bool(has_grace)},
        {format_bool(has_nvlink_c2c)}
    }}"""
        lines.append(entry)
    return ",\n".join(lines)


def generate_header() -> str:
    refresh_capability_cache()
    records = all_capability_records()
    entries = emit_capability_entries(records)
    return f"""#pragma once

// DO NOT EDIT: Generated by core/scripts/utilities/generate_arch_detection_header.py

#include <cuda_runtime.h>
#include <cstdint>
#include <cstdio>
#include <algorithm>
#include <initializer_list>

namespace cuda_arch {{

struct TMALimits {{
    uint32_t max_1d_box_size;
    uint32_t max_2d_box_width;
    uint32_t max_2d_box_height;
}};

inline TMALimits get_tma_limits();

inline int get_max_shared_mem_per_block();

struct ArchitectureLimits {{
    enum class TensorCoreGeneration {{
        None,
        Ampere,
        Hopper,
        Blackwell
    }};

    TMALimits tma{{}};
    int max_shared_mem_per_block = 48 * 1024;
    int max_shared_mem_per_sm = 0;
    int max_threads_per_block = 1024;
    int warp_size = 32;

    bool supports_clusters = false;
    int max_cluster_size = 1;

    TensorCoreGeneration tensor_core_gen = TensorCoreGeneration::None;
    int tensor_tile_m = 64;
    int tensor_tile_n = 64;
    int tensor_tile_k = 16;

    bool has_grace_coherence = false;
    bool has_nvlink_c2c = false;
    size_t kernel_parameter_limit = 4096;
}};

struct TensorCoreTile {{
    int m;
    int n;
    int k;
}};

struct CapabilityData {{
    int major;
    int minor;
    TMALimits tma;
    bool supports_clusters;
    bool has_dsmem;
    int max_cluster_size;
    ArchitectureLimits::TensorCoreGeneration tensor_core_gen;
    int tensor_tile_m;
    int tensor_tile_n;
    int tensor_tile_k;
    bool has_grace_coherence;
    bool has_nvlink_c2c;
}};

inline constexpr CapabilityData kCapabilityTable[] = {{
{entries}
}};

inline const CapabilityData* find_capability(int major, int minor) {{
    for (const auto& entry : kCapabilityTable) {{
        if (entry.major == major && entry.minor == minor) {{
            return &entry;
        }}
    }}
    return nullptr;
}}

inline TMALimits get_tma_limits() {{
    static TMALimits cached_limits = {{0, 0, 0}};
    if (cached_limits.max_1d_box_size != 0) {{
        return cached_limits;
    }}

    cudaDeviceProp props{{}};
    if (cudaGetDeviceProperties(&props, 0) != cudaSuccess) {{
        cached_limits = {{256, 64, 32}};
        return cached_limits;
    }}

    if (const CapabilityData* entry = find_capability(props.major, props.minor)) {{
        cached_limits = entry->tma;
    }} else {{
        cached_limits = {{256, 64, 32}};
    }}
    return cached_limits;
}}

inline int get_max_shared_mem_per_block() {{
    static int cached = -1;
    if (cached >= 0) {{
        return cached;
    }}

    int value = 0;
    if (cudaDeviceGetAttribute(&value, cudaDevAttrMaxSharedMemoryPerBlockOptin, 0) != cudaSuccess ||
        value == 0) {{
        cudaDeviceGetAttribute(&value, cudaDevAttrMaxSharedMemoryPerBlock, 0);
    }}

    if (value == 0) {{
        value = 48 * 1024;
    }}

    cached = value;
    return cached;
}}

inline const ArchitectureLimits& get_architecture_limits() {{
    static ArchitectureLimits cached{{}};
    static bool initialized = false;
    if (initialized) {{
        return cached;
    }}

    cudaDeviceProp props{{}};
    if (cudaGetDeviceProperties(&props, 0) != cudaSuccess) {{
        cached.tma = get_tma_limits();
        initialized = true;
        return cached;
    }}

    cached.tma = get_tma_limits();
    cached.max_shared_mem_per_block = get_max_shared_mem_per_block();
    cudaDeviceGetAttribute(&cached.max_shared_mem_per_sm, cudaDevAttrMaxSharedMemoryPerMultiprocessor, 0);
    cudaDeviceGetAttribute(&cached.max_threads_per_block, cudaDevAttrMaxThreadsPerBlock, 0);
    cudaDeviceGetAttribute(&cached.warp_size, cudaDevAttrWarpSize, 0);

    const CapabilityData* entry = find_capability(props.major, props.minor);
    if (entry) {{
        cached.supports_clusters = entry->supports_clusters;
        cached.max_cluster_size = entry->max_cluster_size;
        cached.tensor_core_gen = entry->tensor_core_gen;
        cached.tensor_tile_m = entry->tensor_tile_m;
        cached.tensor_tile_n = entry->tensor_tile_n;
        cached.tensor_tile_k = entry->tensor_tile_k;
        cached.has_grace_coherence = entry->has_grace_coherence;
        cached.has_nvlink_c2c = entry->has_nvlink_c2c;
        cached.kernel_parameter_limit = entry->has_grace_coherence ? 32768 : 4096;
    }} else {{
        cached.kernel_parameter_limit = (props.major == 12 && props.minor >= 1) ? 32768 : 4096;
        if (props.major >= 10) {{
            cached.supports_clusters = true;
            cached.max_cluster_size = 8;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Blackwell;
            cached.tensor_tile_m = 128;
            cached.tensor_tile_n = 128;
            cached.tensor_tile_k = 64;
        }} else if (props.major == 9) {{
            cached.supports_clusters = true;
            cached.max_cluster_size = 4;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Hopper;
            cached.tensor_tile_m = 128;
            cached.tensor_tile_n = 128;
            cached.tensor_tile_k = 64;
        }} else if (props.major >= 8) {{
            cached.supports_clusters = false;
            cached.max_cluster_size = 1;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Ampere;
            cached.tensor_tile_m = 64;
            cached.tensor_tile_n = 64;
            cached.tensor_tile_k = 32;
        }} else {{
            cached.supports_clusters = false;
            cached.max_cluster_size = 1;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::None;
            cached.tensor_tile_m = 32;
            cached.tensor_tile_n = 32;
            cached.tensor_tile_k = 16;
        }}
        cached.has_grace_coherence = (props.major == 12 && props.minor >= 1);
        cached.has_nvlink_c2c = cached.has_grace_coherence;
    }}

    initialized = true;
    return cached;
}}

inline TensorCoreTile select_tensor_core_tile() {{
    const auto& limits = get_architecture_limits();
    return {{limits.tensor_tile_m, limits.tensor_tile_n, limits.tensor_tile_k}};
}}

template <typename T>
inline int select_square_tile_size(int shared_tiles,
                                   std::initializer_list<int> candidates,
                                   bool enforce_thread_bound = false) {{
    const auto& limits = get_architecture_limits();
    int fallback = *std::min_element(candidates.begin(), candidates.end());

    for (int candidate : candidates) {{
        std::size_t shared_bytes =
            static_cast<std::size_t>(shared_tiles) *
            static_cast<std::size_t>(candidate) *
            static_cast<std::size_t>(candidate) *
            sizeof(T);

        bool fits_shared = shared_bytes <= static_cast<std::size_t>(limits.max_shared_mem_per_block);
        bool fits_threads = !enforce_thread_bound || (candidate * candidate <= limits.max_threads_per_block);

        if (fits_shared && fits_threads) {{
            return candidate;
        }}
    }}

    return fallback;
}}

}}  // namespace cuda_arch
"""


def main() -> None:
    header_text = generate_header()
    HEADER_PATH.write_text(header_text, encoding="utf-8")
    print(f"Wrote {HEADER_PATH}")


if __name__ == "__main__":
    main()
