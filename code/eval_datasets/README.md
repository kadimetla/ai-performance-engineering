# Evaluation Datasets

## Summary
Lightweight synthetic corpora used for fast accuracy and perplexity checks without pulling external datasets. Generated by `core/scripts/utilities/create_eval_datasets.py`.

## Learning Goals
- Reuse deterministic token streams to compare precision modes and kernels.
- Sanity-check perplexity and routing behaviors before touching larger corpora.
- Regenerate fixtures quickly with reproducible shapes and vocab sizes.

## Directory Layout
| Path | Description |
| --- | --- |
| `random_tokens.txt` | Uniform random tokens for stress-testing noise tolerance. |
| `structured_tokens.txt` | Zipf-ish mix with 70% common / 30% rare tokens. |
| `repetitive_tokens.txt` | Repeating 100-token pattern for overfitting/regression checks. |
| `arithmetic.txt` | Simple arithmetic expressions encoded as tokens for reasoning sanity tests. |

## Running / Usage
- Run perplexity checks: `python ch16/perplexity_eval.py eval_datasets/random_tokens.txt --output-json artifacts/perplexity_random.json`
- Compare precision/accuracy: `python core/scripts/utilities/compare_precision_accuracy.py --dataset eval_datasets/structured_tokens.txt`
- Regenerate with custom sizes:
  ```bash
  python core/scripts/utilities/create_eval_datasets.py --output-dir eval_datasets --num-tokens 5000 --vocab-size 10000
  ```

## Validation Checklist
- Regenerating with the command above rewrites all four files and this README in-place.
- Perplexity eval emits JSON with prompt/decode percentiles and should run without downloads.
- `core/scripts/utilities/compare_precision_accuracy.py` completes against each file when the harness is installed.

## Notes
- Files are kept small to allow quick correctness runs inside CI and notebook environments.
- Token counts/vocab sizes are driven by the generator flags; keep them consistent when updating expectations.
