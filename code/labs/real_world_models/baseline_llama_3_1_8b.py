#!/usr/bin/env python3
"""Baseline: Llama 3.1 8B without optimizations.

Standard PyTorch eager mode without compile, FP8, or FlexAttention.
"""

from __future__ import annotations

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from typing import Optional

import torch

from core.benchmark.verification_mixin import VerificationPayloadMixin
from core.harness.benchmark_harness import BaseBenchmark, BenchmarkConfig
from labs.real_world_models.llama_3_1_8b_optimization import Llama31_8B_Optimization


class BaselineLlama31_8B(VerificationPayloadMixin, BaseBenchmark):
    """Baseline Llama 3.1 8B - eager mode, no optimizations."""

    def __init__(self, batch_size: int = 1, seq_length: int = 2048):
        super().__init__()
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.model_wrapper = None
        self.output: Optional[torch.Tensor] = None
        self.parameter_count = 0
        self._last_metrics: dict = {}
        self.register_workload_metadata(requests_per_iteration=float(batch_size))

    def setup(self) -> None:
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(42)
        self.model_wrapper = Llama31_8B_Optimization(
            batch_size=self.batch_size,
            seq_length=self.seq_length,
            use_compile=False,  # No torch.compile
            use_fp8=False,      # No FP8
            use_flex_attention=False,  # No FlexAttention
        )
        self.model_wrapper.setup()
        self.parameter_count = sum(p.numel() for p in self.model_wrapper.layers.parameters())

    def benchmark_fn(self) -> None:
        if self.model_wrapper is None:
            raise RuntimeError("Model wrapper not initialized")
        elapsed_ms = self.model_wrapper.run()
        self._last_metrics = {"elapsed_ms": float(elapsed_ms)}
        self.output = self.model_wrapper.output
        self._synchronize()
        if self.output is None:
            raise RuntimeError("benchmark_fn() did not produce output")

    def capture_verification_payload(self) -> None:
        self._set_verification_payload(
            inputs={"input": self.model_wrapper.input.detach()},
            output=self.output,
            batch_size=self.batch_size,
            parameter_count=self.parameter_count,
            precision_flags={"bf16": True, "fp16": False, "fp8": False, "tf32": torch.backends.cuda.matmul.allow_tf32},
            output_tolerance=(0.1, 1.0),
        )

    def teardown(self) -> None:
        if self.model_wrapper:
            self.model_wrapper.teardown()
        self.model_wrapper = None
        super().teardown()

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(iterations=10, warmup=5)

    def get_custom_metrics(self) -> dict:
        return self._last_metrics


def get_benchmark() -> BaseBenchmark:
    return BaselineLlama31_8B()


if __name__ == "__main__":
    from core.harness.benchmark_harness import benchmark_main
    benchmark_main(get_benchmark)
