#!/usr/bin/env python3
"""Optimized: FSDP2 with FP8 training on Blackwell.

Advanced FSDP2 training with:
- FP8 mixed precision via Transformer Engine
- Gradient checkpointing
- Communication overlap
- Optimal sharding strategy for Blackwell
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import MixedPrecision
from typing import Dict, Any
import sys
from pathlib import Path
import time
import os

# Add common to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from core.benchmark.verification import simple_signature
from core.harness.benchmark_harness import (
    BaseBenchmark,
    BenchmarkConfig,
    BenchmarkHarness,
    BenchmarkMode,
)
from core.utils.logger import get_logger

logger = get_logger(__name__)

# Check for torchao (FP8 support)
try:
    from torchao.float8 import convert_to_float8_training, Float8LinearConfig
    TORCHAO_AVAILABLE = True
except ImportError:
    TORCHAO_AVAILABLE = False
    logger.warning("torchao not available, using BF16 fallback")


class OptimizedFSDP2FP8(BaseBenchmark):
    """Optimized FSDP2 with FP8 training."""

    def __init__(
        self,
        batch_size: int = 8,  # 2Ã— baseline
        seq_length: int = 2048,
        hidden_size: int = 4096,
        num_layers: int = 8,
        micro_batch_size: int = 2,
        use_fp8: bool = True,
    ):
        super().__init__()
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.micro_batch_size = micro_batch_size
        self.use_fp8 = use_fp8 and TORCHAO_AVAILABLE
        self._last_metrics: Dict[str, float] = {}
        self.register_workload_metadata(requests_per_iteration=float(batch_size))

        # Initialize distributed
        self._init_distributed()

        self.device = torch.device(f"cuda:{self.local_rank}")
        torch.cuda.set_device(self.device)

        logger.info(
            f"FSDP2 Rank {self.rank}/{self.world_size}: "
            f"{'FP8' if self.use_fp8 else 'BF16'} optimized"
        )

    def _init_distributed(self):
        """Initialize distributed."""
        if not dist.is_initialized():
            if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
                dist.init_process_group(backend='nccl')
            else:
                logger.warning("Simulation mode")
                self.rank = 0
                self.world_size = 1
                self.local_rank = 0
                return
        
        self.rank = dist.get_rank()
        self.world_size = dist.get_world_size()
        self.local_rank = self.rank % torch.cuda.device_count()

    def setup(self):
        """Initialize FSDP2 model with FP8."""
        if getattr(self, "world_size", 1) < 2:
            raise RuntimeError("SKIPPED: requires >=2 GPUs")
        self.register_workload_metadata(
            tokens_per_iteration=float(self.batch_size * self.seq_length),
        )

        # Create model
        class SimpleTransformerLayer(nn.Module):
            def __init__(self, hidden_size):
                super().__init__()
                self.linear1 = nn.Linear(hidden_size, hidden_size * 4, bias=False)
                self.linear2 = nn.Linear(hidden_size * 4, hidden_size, bias=False)
                self.norm = nn.LayerNorm(hidden_size)

            def forward(self, x):
                return x + self.linear2(torch.relu(self.linear1(self.norm(x))))

        model = nn.ModuleList([
            SimpleTransformerLayer(self.hidden_size)
            for _ in range(self.num_layers)
        ]).to(self.device)

        # Convert to FP8 if available
        if self.use_fp8:
            logger.info("Converting to FP8 training...")
            fp8_config = Float8LinearConfig(
                enable_fsdp_float8_all_gather=True,  # FP8 all-gather
                enable_pre_and_post_forward=True,
            )
            convert_to_float8_training(model, config=fp8_config)

        # Wrap with FSDP2
        mixed_precision = MixedPrecision(
            param_dtype=torch.bfloat16,
            reduce_dtype=torch.bfloat16,
            buffer_dtype=torch.bfloat16,
        )

        self.model = FSDP(
            model,
            mixed_precision=mixed_precision,
            use_orig_params=True,
        )

        # Create input
        self.input = torch.randn(
            self.micro_batch_size,
            self.seq_length,
            self.hidden_size,
            device=self.device,
            dtype=torch.bfloat16
        )

        # Fused AdamW optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            fused=True  # Optimized for Blackwell
        )

        logger.info(f"FSDP2 + FP8 setup complete (Rank {self.rank})")

    def benchmark_fn(self) -> None:
        """Execute optimized FSDP2 + FP8 training."""
        torch.cuda.synchronize(self.device)
        torch.cuda.reset_peak_memory_stats(self.device)
        start = time.perf_counter()

        # Training step with gradient accumulation
        grad_accum_steps = self.batch_size // self.micro_batch_size

        self.optimizer.zero_grad()

        for _ in range(grad_accum_steps):
            # Forward
            output = self.input
            for layer in self.model:
                output = layer(output)

            # Loss (scaled for gradient accumulation)
            loss = output.mean() / grad_accum_steps

            # Backward
            loss.backward()

        # Optimizer step (with gradient clipping)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()

        self._synchronize()
        elapsed = time.perf_counter() - start

        # Memory metrics
        peak_memory_gb = torch.cuda.max_memory_allocated(self.device) / (1024**3)

        # Calculate throughput
        tokens_per_sec = (self.batch_size * self.seq_length) / elapsed

        logger.info(f"Rank {self.rank}: {elapsed*1000:.2f} ms, {tokens_per_sec:.0f} tok/s")
        logger.info(f"Peak memory: {peak_memory_gb:.2f} GB")

        self._last_metrics = {
            "latency_ms": elapsed * 1000,
            "peak_memory_gb": peak_memory_gb,
            "tokens_per_sec": tokens_per_sec,
            "loss": loss.item() * grad_accum_steps,
        }

    def get_custom_metrics(self) -> Dict[str, float]:
        """Expose last-run metrics to the harness."""
        return self._last_metrics

    def teardown(self):
        """Clean up resources."""
        del self.model, self.optimizer, self.input
        super().teardown()

    def get_verify_output(self) -> torch.Tensor:
        """Return output tensor for verification comparison."""
        raise RuntimeError("SKIPPED: torchrun/FSDP2 benchmark verification not applicable")

    def get_input_signature(self) -> dict:
        """Return input signature for verification."""
        return simple_signature(
            batch_size=self.batch_size,
            dtype="float8_e4m3fn",
            precision_flags={"fp8": True, "bf16": False, "tf32": False},
            seq_length=self.seq_length,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            micro_batch_size=self.micro_batch_size,
            world_size=self.world_size,
        )

    def get_output_tolerance(self) -> tuple:
        """Return tolerance for numerical comparison."""
        return (0.1, 1.0)


def run_benchmark(
    batch_size: int = 8,
    seq_length: int = 2048,
    hidden_size: int = 4096,
    num_layers: int = 8,
    micro_batch_size: int = 2,
    use_fp8: bool = True,
    profile: str = "none",
    **kwargs
) -> Dict[str, Any]:
    """Run optimized FSDP2 + FP8 benchmark."""

    benchmark = OptimizedFSDP2FP8(
        batch_size=batch_size,
        seq_length=seq_length,
        hidden_size=hidden_size,
        num_layers=num_layers,
        micro_batch_size=micro_batch_size,
        use_fp8=use_fp8,
    )

    config = BenchmarkConfig(
        iterations=3,
        warmup=5,
        profile_mode=profile,
    )

    harness = BenchmarkHarness(mode=BenchmarkMode.TRAINING, config=config)

    result = harness.benchmark(benchmark, name="optimized_fsdp2_fp8")

    metrics = result.custom_metrics or {}
    return {
        "mean_time_ms": result.timing.mean_ms,
        "peak_memory_gb": metrics.get("peak_memory_gb"),
        "tokens_per_sec": metrics.get("tokens_per_sec"),
        "loss": metrics.get("loss"),
        "precision": "fp8" if benchmark.use_fp8 else "bf16",
    }


def get_benchmark() -> BaseBenchmark:
    """Factory function for benchmark discovery."""
    return OptimizedFSDP2FP8()


if __name__ == "__main__":
    from core.harness.benchmark_harness import benchmark_main
    benchmark_main(get_benchmark)
